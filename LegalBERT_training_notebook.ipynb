{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3090,"status":"ok","timestamp":1674669001082,"user":{"displayName":"Thanet Markchom","userId":"00244922274630544136"},"user_tz":0},"id":"yMa-OLFKidua","outputId":"f83d45c3-53cf-4da2-eaf5-8b19d329008b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4617,"status":"ok","timestamp":1674669005686,"user":{"displayName":"Thanet Markchom","userId":"00244922274630544136"},"user_tz":0},"id":"zjfUfKu2JrTo","outputId":"872e0f89-2386-4f46-e75b-12ca07158f75"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1674669005687,"user":{"displayName":"Thanet Markchom","userId":"00244922274630544136"},"user_tz":0},"id":"0HWslNrUJdla","outputId":"49c3f87b-d9b8-4ab4-d681-09f50db5fa7d"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import os\n","import re\n","import random\n","import pandas as pd\n","import numpy as np\n","import csv\n","import tensorflow as tf\n","import torch\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","import textwrap\n","import progressbar\n","import keras\n","from keras_preprocessing.sequence import pad_sequences\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","import time\n","import datetime\n","import json\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize\n","\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix\n","\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","    \n","def metrics_calculator(preds, test_labels):\n","    cm = confusion_matrix(test_labels, preds)\n","    TP = []\n","    FP = []\n","    FN = []\n","    for i in range(0,2):\n","        summ = 0\n","        for j in range(0,2):\n","            if(i!=j):\n","                summ=summ+cm[i][j]\n","\n","        FN.append(summ)\n","    for i in range(0,2):\n","        summ = 0\n","        for j in range(0,2):\n","            if(i!=j):\n","                summ=summ+cm[j][i]\n","\n","        FP.append(summ)\n","    for i in range(0,2):\n","        TP.append(cm[i][i])\n","    precision = []\n","    recall = []\n","    for i in range(0,2):\n","        precision.append(TP[i]/(TP[i] + FP[i]))\n","        recall.append(TP[i]/(TP[i] + FN[i]))\n","\n","    macro_precision = sum(precision)/2\n","    macro_recall = sum(recall)/2\n","    micro_precision = sum(TP)/(sum(TP) + sum(FP))\n","    micro_recall = sum(TP)/(sum(TP) + sum(FN))\n","    micro_f1 = (2*micro_precision*micro_recall)/(micro_precision + micro_recall)\n","    macro_f1 = (2*macro_precision*macro_recall)/(macro_precision + macro_recall)\n","    return macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1\n","\n","seed_val = 2212\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7Q9PNnweVuf"},"outputs":[],"source":["# load all models and select roberta\n","from transformers import AutoTokenizer, AutoModel, BertForSequenceClassification\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cxShnTP4islX"},"outputs":[],"source":["#save the trained model\n","\n","output_dir     = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Models/finetuned/legalbert_trained_on_single/'\n","pretrained_dir = None\n","\n","input_file = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/ILDC_single_train_dev.csv'\n","#test_file = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/c2_test.csv'\n","#pred_file = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/legalbert_trained_on_single_preprocessed_2_fixed_first510tokens_10eps_c2_test.csv'\n","\n","\n","\n","#input_file = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/ILDC_single_train_dev.csv'\n","\n","#test_file  = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/ILDC_single_test_preprocessed_2.csv'\n","#pred_file  = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/ILDC_single_test_preprocessed_2_preds.csv'\n","#test_file  = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/ILDC_single_test_explanation_preprocessed_2.csv'\n","#pred_file  = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/ILDC_single_test_explanation_preprocessed_2_preds.csv'\n","\n","\n","#test_file  = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/ILDC_single_test_explanation.csv'\n","#pred_file  = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/ILDC_single_test_explanation_preds.csv'\n","\n","#test_exp_file  = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/ILDC_single_test_explanation.csv'\n","#pred_exp_file  = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/ILDC_single_test_explanation_pred.csv'\n","\n","\n","\n","#load data\n","df = pd.read_csv(input_file) # path to multi_dataset\n","df = df.applymap(lambda x: x.strip().replace('-\\n', '').replace('\\n', '') if isinstance(x, str) else x)\n","train_set = df.query(\" split=='train' \")\n","#validation_set = df.query(\" split=='dev' \")\n","validation_set = pd.read_csv(test_file)\n","#test_set = pd.read_csv(test_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9178,"status":"ok","timestamp":1674669018390,"user":{"displayName":"Thanet Markchom","userId":"00244922274630544136"},"user_tz":0},"id":"4IaMjXbWvbCG","outputId":"14ed72f0-aca0-47fc-9b73-f8d0953c25b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Load from  /content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Models/finetuned/legalbert_trained_on_single_preprocessed_2_fixed_first510tokens_10eps/\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":25}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n","\n","\n","if pretrained_dir == None:\n","  print('\\n\\nNo pretrained model')  \n","  model = BertForSequenceClassification.from_pretrained(\"nlpaueb/legal-bert-base-uncased\", output_attentions=True)\n","\n","else:\n","  model = BertForSequenceClassification.from_pretrained(pretrained_dir)\n","  print('\\n\\nLoad from ', pretrained_dir)\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hBjxqwueJ0pq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":467,"status":"ok","timestamp":1674669018797,"user":{"displayName":"Thanet Markchom","userId":"00244922274630544136"},"user_tz":0},"id":"W8DoEiiw5gNf","outputId":"86e92258-3b6a-4862-d04f-152668bf6d83"},"outputs":[{"output_type":"stream","name":"stdout","text":["    Unnamed: 0                                   uid  t_ind  s_ind  \\\n","0            0  c0e246e7-60f3-4523-8da7-056eb7887c1d      1      0   \n","1            1  47a5abfc-0258-4a1f-89bb-8ea70d8d466b      1      0   \n","2            2  16860b14-866f-4cb5-a82d-50bac0f0d2ff      1      0   \n","3            3  c455ccbf-3390-475f-b4e9-f72bce15a2cf      1      0   \n","4            4  b8cdf7a4-489a-460b-a511-16eae79b8912      1      0   \n","5            5  4ae07c84-2c72-455b-8154-587196becc0c      1      0   \n","6            6  2280d8b2-1223-44fc-bea0-577e2e1dfb4a      1      0   \n","7            7  b4035da9-0319-4ec1-b599-3ab987dc5b64      1      0   \n","8            8  14b24130-5ac7-4ba7-8fe0-cac06e59d6ab      1      0   \n","9            9  de4b65c9-ba8c-4f71-bd34-f6548db71cbe      1      0   \n","10          10  8ea3dc45-ac1f-4ddf-adfa-ed3fc6d425ab      1      0   \n","11          11  27d46dda-ba34-4fd6-bd27-f95861d49726      1      0   \n","12          12  40ce6f70-971f-4640-9fbc-00ce4dc948bf      1      0   \n","13          13  0287f148-fb34-454b-a028-fbcb9cd361f7      1      0   \n","14          14  fb77c2b9-ac2d-441d-b939-176356006db1      1      0   \n","15          15  a3ba1b9d-45c5-4d75-8d66-a86b118ef074      1      0   \n","16          16  51272425-4820-49d7-bc8d-3f933f8d12cd      1      0   \n","17          17  91bbea8e-9155-4a48-8596-47a56627c4e9      1      0   \n","18          18  5285944b-beef-4d9d-bc8d-e11330e1d7a5      1      0   \n","19          19  cbc67de2-3a6f-4db9-9925-d828335dea93      1      0   \n","20          20  05e9b3eb-0728-4d00-b5fe-3f4f39cb4b17      1      0   \n","21          21  a22a391d-1a10-4cb2-bd16-f9b974326a03      1      0   \n","22          22  bbe0f8e3-a8d2-4356-a173-33970371a8f6      1      0   \n","23          23  e3d74335-f000-4cc7-8636-1de82c57bec0      1      0   \n","24          24  62fecfa4-55a7-44e9-bb3f-411278d41091      1      0   \n","25          25  5e3f38ac-402d-426b-a4e9-c61cc8df228d      1      0   \n","26          26  03127bf2-3baf-4932-adc4-22ed9c3c916f      1      0   \n","27          27  bb371fa3-32d4-43ca-a536-abd0525fb095      1      0   \n","28          28  3154923a-216d-46f2-abdf-64ac0c918a14      1      0   \n","29          29  6996e7ba-5ddf-49b3-a963-b9fd47ecb61c      1      0   \n","30          30  8899e621-209f-425a-adde-f94028f34a5e      1      0   \n","31          31  b060244a-b137-488e-91fe-8470bc2acca6      1      0   \n","32          32  4d1453c2-c94a-4785-b6e2-3ea67164c30c      1      0   \n","33          33  7bf93c07-b898-40b2-a14d-cbfd3e253dd9      1      0   \n","34          34  23153306-85f7-4f69-8370-83535c3a4519      1      0   \n","35          35  9abc8955-86ce-4d46-8f02-b7ac092542a1      1      0   \n","36          36  0fe479be-8210-441f-a7be-d85cd4595f6e      1      0   \n","37          37  49a29894-e619-4912-97e0-e993cf59e956      1      0   \n","38          38  cf010167-f220-4bb8-bd4b-9cd6e9c717f4      1      0   \n","39          39  2db80712-5b15-4494-8a80-30de1c702ee6      1      0   \n","40          40  ff8d7dc7-c579-447f-a85c-af39024aaa34      1      0   \n","41          41  fdba5bdd-a058-4909-8e69-be6c24260654      1      0   \n","42          42  e7a333a6-fe98-4a49-baa7-570c38c1e6d0      1      0   \n","43          43  26a90565-dea5-4c37-b9af-b81eff27266f      1      0   \n","44          44  b7be3a25-698b-4ead-8559-1d5ace58dc96      1      0   \n","45          45  20ccb2dd-434d-4968-8eef-cf906c7a9de5      1      0   \n","46          46  eb30a124-6b88-429f-b863-6234df93d59d      1      0   \n","47          47  2ef07052-d43e-4a1d-b9eb-a625c3dcc573      1      0   \n","48          48  d5e9efdb-72c8-4622-aec2-9970b2f255ad      1      0   \n","49          49  b6f8d42b-c2d3-4c28-8ac2-c4dc694f5ec7      1      0   \n","\n","                                                 text  id  \n","0   or an equivalent degree of traditional monasti...   0  \n","1   by order of the governumber of bihar sd. accor...   1  \n","2   of appointment. we cannumber allow mr. g to go...   2  \n","3   this provision obviously applies only to situa...   3  \n","4   misra and ms. a subhashini p. for the responde...   4  \n","5   it seems to us manifestly clear that the saran...   5  \n","6   beyond the absence in o. ix r. 9 of the words ...   6  \n","7   the learned magistrate sent that application t...   7  \n","8   several grounds are specified such as failure ...   8  \n","9   Engineer, W.R.Circle, Jagdalpur in a vacant po...   9  \n","10  this date was to be the date in london on decl...  10  \n","11  400/- in default rigorous imprisonment for thr...  11  \n","12  he remanded the case to the district deputy co...  12  \n","13  we find numberforce in other submissions. in s...  13  \n","14  267. such areas as the companymissioner of inc...  14  \n","15  having regard to the fact that the decision ab...  15  \n","16  11b on the ground that it violates art. 11b is...  16  \n","17  m. number 180 of 1949 in c. a. number 82 of 19...  17  \n","18  61/90 on the file of the Munsiff No. Procedure...  18  \n","19  ram reddy and a. v. v. nair for the respondent...  19  \n","20  this was number to the liking of the municipal...  20  \n","21  the tribunal rejected the applications for rev...  21  \n","22  the petition was heard by a learned single jud...  22  \n","23  tageous and with all the powers of absolute ow...  23  \n","24  since the qualifying event occurs on the death...  24  \n","25  679 and 680 of 1957. appeals by special leave ...  25  \n","26  page 597 art. the expression such companyditio...  26  \n","27  Thus, the learned Single Judge allowed the wri...  27  \n","28  misra advocate-general for the state of u. p. ...  28  \n","29  226 of the constitution in the high companyrt ...  29  \n","30  hardayal hardy for the respondents. the high c...  30  \n","31  the assessee took the matter in appeal to the ...  31  \n","32  the companymissioner has appealed against the ...  32  \n","33  on february 12 1947 messrs sewa santokh brothe...  33  \n","34  k. jain b.p. singh and anjeet kumar for the re...  34  \n","35  the companyrt it is true will direct. we will ...  35  \n","36  the appellant thereupon preferred an appeal to...  36  \n","37  it would seem therefore that the share of whic...  37  \n","38  from the judgment and order dated 8.11.1978 of...  38  \n","39  for enhancement of the sentence. and to make a...  39  \n","40  krishnamani vineet kumar mohan k.c. he urged t...  40  \n","41  on these findings he acquitted all the accused...  41  \n","42  in july. there is thus good ground for thinkin...  42  \n","43  since the award of solatium is in companysider...  43  \n","44  per annum on the amount of such deposit in add...  44  \n","45  satish chandra aggarwal s.k. sulochna devi was...  45  \n","46  the date on which the amendments made by the f...  46  \n","47  it commenced on the first day of the month and...  47  \n","48  mahajan j.--i companycur in the judgment which...  48  \n","49  girish chandra for the respondents. 15.3.1985 ...  49  \n"]}],"source":["validation_set['id'] = range(len(validation_set))\n","print(validation_set)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1674669019222,"user":{"displayName":"Thanet Markchom","userId":"00244922274630544136"},"user_tz":0},"id":"N4rRfixgwOnw","outputId":"de8db009-1b20-4cc0-a90c-cc85077220b9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["      Unnamed: 0                                               text  t_ind  \\\n","0              0  the manzil has a ground floor and three other ...      1   \n","1              1  radha bai as her licensee. radha bai. it was s...      1   \n","2              2  he retired in 1973. the rent has been paid sin...      1   \n","3              3  the fact that the returning officer rejected t...      1   \n","4              4  2 to 4.30 for rolling 1000 pieces. women and i...      1   \n","...          ...                                                ...    ...   \n","5971        5971  the clinic was set up at bhilai somewhere in 1...      1   \n","5972        5972  100. the state preferred appeals to the high c...      1   \n","5973        5973  the licensee had numbervested right to a renew...      1   \n","5974        5974  the plaintiff moved the high companyrt in revi...      1   \n","5975        5975  in respect of the first offence it provides fo...      1   \n","\n","      s_ind  label  split    id  \n","0         0      1  train     1  \n","1         0      1  train     2  \n","2         0      0  train     3  \n","3         0      1  train     4  \n","4         0      0  train     5  \n","...     ...    ...    ...   ...  \n","5971      0      1    dev  5972  \n","5972      0      0    dev  5973  \n","5973      0      0    dev  5974  \n","5974      0      1    dev  5975  \n","5975      0      0    dev  5976  \n","\n","[5976 rows x 7 columns]"],"text/html":["\n","  <div id=\"df-ffbd132d-509f-425d-b691-4c148b841682\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>text</th>\n","      <th>t_ind</th>\n","      <th>s_ind</th>\n","      <th>label</th>\n","      <th>split</th>\n","      <th>id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>the manzil has a ground floor and three other ...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>train</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>radha bai as her licensee. radha bai. it was s...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>train</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>he retired in 1973. the rent has been paid sin...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>train</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>the fact that the returning officer rejected t...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>train</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>2 to 4.30 for rolling 1000 pieces. women and i...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>train</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5971</th>\n","      <td>5971</td>\n","      <td>the clinic was set up at bhilai somewhere in 1...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>dev</td>\n","      <td>5972</td>\n","    </tr>\n","    <tr>\n","      <th>5972</th>\n","      <td>5972</td>\n","      <td>100. the state preferred appeals to the high c...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>dev</td>\n","      <td>5973</td>\n","    </tr>\n","    <tr>\n","      <th>5973</th>\n","      <td>5973</td>\n","      <td>the licensee had numbervested right to a renew...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>dev</td>\n","      <td>5974</td>\n","    </tr>\n","    <tr>\n","      <th>5974</th>\n","      <td>5974</td>\n","      <td>the plaintiff moved the high companyrt in revi...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>dev</td>\n","      <td>5975</td>\n","    </tr>\n","    <tr>\n","      <th>5975</th>\n","      <td>5975</td>\n","      <td>in respect of the first offence it provides fo...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>dev</td>\n","      <td>5976</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5976 rows Ã— 7 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ffbd132d-509f-425d-b691-4c148b841682')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ffbd132d-509f-425d-b691-4c148b841682 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ffbd132d-509f-425d-b691-4c148b841682');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":27}],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"pmfUxc93wOyR"},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zANK4YJ0LZEf"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"_Gr42aW0DQBe"},"source":["# Preparing data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqBkaIbDKMlX"},"outputs":[],"source":["def input_id_maker(dataf, tokenizer):\n","  input_ids = []\n","  lengths = []\n","\n","  for i in progressbar.progressbar(range(len(dataf['text']))):\n","    sen = dataf['text'].iloc[i]\n","    sen = tokenizer.tokenize(sen)\n","    CLS = tokenizer.cls_token\n","    SEP = tokenizer.sep_token\n","    if(len(sen) > 510):\n","      sen = sen[:510]\n","    \n","    '''if len(sen) > 510:\n","      sen = sen[0:255] + sen[len(sen)-255:]'''\n","\n","    sen = [CLS] + sen + [SEP]\n","    encoded_sent = tokenizer.convert_tokens_to_ids(sen)\n","    input_ids.append(encoded_sent)\n","    lengths.append(len(encoded_sent))\n","\n","  input_ids = pad_sequences(input_ids, maxlen=512, value=0, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","  return input_ids, lengths"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ctB5AycbKVj6","outputId":"0abfd1fb-5242-4d14-94ce-016325d224ed","executionInfo":{"status":"ok","timestamp":1674669138203,"user_tz":0,"elapsed":118771,"user":{"displayName":"Thanet Markchom","userId":"00244922274630544136"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["N/A% (0 of 4982) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--Token indices sequence length is longer than the specified maximum sequence length for this model (1800 > 512). Running this sequence through the model will result in indexing errors\n","100% (4982 of 4982) |####################| Elapsed Time: 0:01:57 Time:  0:01:57\n","100% (50 of 50) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"]}],"source":["train_input_ids, train_lengths = input_id_maker(train_set, tokenizer)\n","validation_input_ids, validation_lengths = input_id_maker(validation_set, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c_rrz5-vKX_x"},"outputs":[],"source":["def att_masking(input_ids):\n","  attention_masks = []\n","  for sent in input_ids:\n","    att_mask = [int(token_id > 0) for token_id in sent]\n","    attention_masks.append(att_mask)\n","  return attention_masks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TDFGL8XPKZ86"},"outputs":[],"source":["train_attention_masks = att_masking(train_input_ids)\n","validation_attention_masks = att_masking(validation_input_ids)\n","\n","train_labels = train_set['label'].to_numpy().astype('int')\n","#validation_labels = validation_set['label'].to_numpy().astype('int')\n","validation_labels = validation_set['id'].to_numpy().astype('int')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1UJt0WJKfAR"},"outputs":[],"source":["train_inputs = train_input_ids\n","validation_inputs = validation_input_ids\n","train_masks = train_attention_masks\n","validation_masks = validation_attention_masks\n","\n","train_inputs = torch.tensor(train_inputs)\n","train_labels = torch.tensor(train_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_inputs = torch.tensor(validation_inputs)\n","validation_labels = torch.tensor(validation_labels)\n","validation_masks = torch.tensor(validation_masks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fNCXSakabP_u"},"outputs":[],"source":["batch_size = 6\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size = batch_size)\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size = batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OK2uGb9qypeO"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"R1-7D6UxDKPv"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"nyr_rKJDKqX1","executionInfo":{"status":"ok","timestamp":1674669139376,"user_tz":0,"elapsed":36,"user":{"displayName":"Thanet Markchom","userId":"00244922274630544136"}},"outputId":"72c22b15-754f-4829-dc77-0f143b128e5d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'# max batch size should be 6 due to colab limits\\nbatch_size = 6\\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\\ntrain_sampler = RandomSampler(train_data)\\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size = batch_size)\\n\\nlr = 2e-6\\nmax_grad_norm = 1.0\\nepochs = 10\\nnum_total_steps = len(train_dataloader)*epochs\\nnum_warmup_steps = 1000\\nwarmup_proportion = float(num_warmup_steps) / float(num_total_steps)  # 0.1\\noptimizer = AdamW(model.parameters(), lr=lr, correct_bias=True)\\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = num_total_steps)\\n\\nloss_values = []\\n\\n# For each epoch...\\nfor epoch_i in range(0, epochs):\\n    print(\\'======== Epoch {:} / {:} ========\\'.format(epoch_i + 1, epochs))\\n    print(\\'Training...\\')\\n\\n    t0 = time.time()\\n    total_loss = 0\\n\\n    model.train()\\n\\n    for step, batch in enumerate(train_dataloader):\\n        if step % 40 == 0 and not step == 0:\\n            print(\\'  Batch {:>5,}  of  {:>5,}. \\'.format(step, len(train_dataloader)))\\n\\n        \\n        b_input_ids = batch[0].to(device)\\n        b_input_mask = batch[1].to(device)\\n        b_labels = batch[2].to(device)\\n\\n        model.zero_grad()        \\n\\n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\\n        \\n        loss = outputs[0]\\n        total_loss += loss.item()\\n        loss.backward()\\n\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\\n\\n        optimizer.step()\\n        scheduler.step()\\n\\n    avg_train_loss = total_loss / len(train_dataloader)            \\n    loss_values.append(avg_train_loss)\\n\\n    print(\"\")\\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\\n\\n    if (epoch_i+1)%1 == 0:\\n    # Create output directory if needed\\n      if not os.path.exists(output_dir):\\n          os.makedirs(output_dir)\\n\\n      print(\"Saving model to %s (epoch %d)\" % (output_dir, epoch_i+1))\\n      model_to_save = model.module if hasattr(model, \\'module\\') else model  # Take care of distributed/parallel training\\n      model_to_save.save_pretrained(output_dir)\\n      tokenizer.save_pretrained(output_dir)\\n\\n\\n    if (epoch_i+1)%5 == 0:\\n        \\n      print(\"\")\\n      print(\"Running Validation...\")\\n\\n      t0 = time.time()\\n\\n      model.eval()\\n\\n      eval_loss, eval_accuracy = 0, 0\\n      nb_eval_steps, nb_eval_examples = 0, 0\\n\\n      for batch in validation_dataloader:\\n          batch = tuple(t.to(device) for t in batch)\\n          b_input_ids, b_input_mask, b_labels = batch\\n          \\n          with torch.no_grad():        \\n            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\\n      \\n          logits = outputs[0]\\n\\n          logits = logits.detach().cpu().numpy()\\n          label_ids = b_labels.to(\\'cpu\\').numpy()\\n          \\n          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\\n          eval_accuracy += tmp_eval_accuracy\\n\\n          nb_eval_steps += 1\\n\\n      # Report the final accuracy for this validation run.\\n      print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\\n\\nprint(\"\")\\nprint(\"Training complete!\")\\n\\n\\nif not os.path.exists(output_dir):\\n  os.makedirs(output_dir)\\n\\nprint(\"Saving model to %s (epoch %d)\" % (output_dir, epoch_i+1))\\nmodel_to_save = model.module if hasattr(model, \\'module\\') else model  # Take care of distributed/parallel training\\nmodel_to_save.save_pretrained(output_dir)\\ntokenizer.save_pretrained(output_dir)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":34}],"source":["'''# max batch size should be 6 due to colab limits\n","batch_size = 6\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size = batch_size)\n","\n","lr = 2e-6\n","max_grad_norm = 1.0\n","epochs = 10\n","num_total_steps = len(train_dataloader)*epochs\n","num_warmup_steps = 1000\n","warmup_proportion = float(num_warmup_steps) / float(num_total_steps)  # 0.1\n","optimizer = AdamW(model.parameters(), lr=lr, correct_bias=True)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = num_total_steps)\n","\n","loss_values = []\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    t0 = time.time()\n","    total_loss = 0\n","\n","    model.train()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        if step % 40 == 0 and not step == 0:\n","            print('  Batch {:>5,}  of  {:>5,}. '.format(step, len(train_dataloader)))\n","\n","        \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        model.zero_grad()        \n","\n","        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","        \n","        loss = outputs[0]\n","        total_loss += loss.item()\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    loss_values.append(avg_train_loss)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","\n","    if (epoch_i+1)%1 == 0:\n","    # Create output directory if needed\n","      if not os.path.exists(output_dir):\n","          os.makedirs(output_dir)\n","\n","      print(\"Saving model to %s (epoch %d)\" % (output_dir, epoch_i+1))\n","      model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","      model_to_save.save_pretrained(output_dir)\n","      tokenizer.save_pretrained(output_dir)\n","\n","\n","    if (epoch_i+1)%5 == 0:\n","        \n","      print(\"\")\n","      print(\"Running Validation...\")\n","\n","      t0 = time.time()\n","\n","      model.eval()\n","\n","      eval_loss, eval_accuracy = 0, 0\n","      nb_eval_steps, nb_eval_examples = 0, 0\n","\n","      for batch in validation_dataloader:\n","          batch = tuple(t.to(device) for t in batch)\n","          b_input_ids, b_input_mask, b_labels = batch\n","          \n","          with torch.no_grad():        \n","            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","      \n","          logits = outputs[0]\n","\n","          logits = logits.detach().cpu().numpy()\n","          label_ids = b_labels.to('cpu').numpy()\n","          \n","          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","          eval_accuracy += tmp_eval_accuracy\n","\n","          nb_eval_steps += 1\n","\n","      # Report the final accuracy for this validation run.\n","      print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","\n","if not os.path.exists(output_dir):\n","  os.makedirs(output_dir)\n","\n","print(\"Saving model to %s (epoch %d)\" % (output_dir, epoch_i+1))\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PpacOMWKW8H-"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ZL4jYL_vDEej"},"source":["#Predicting"]},{"cell_type":"markdown","metadata":{"id":"1-nrvVp0o7fy"},"source":["## Evaluation on training set\n","\n","**last 510 tokens**\n","\n","**preprocessed_2**\n","\n","10 epochs\n","Predicting labels for 4,982 test sentences...\n","0.9411882778000803\n","0.9515030558839894 0.9257865202796357 0.9384686457828602 0.9411882778000803 0.9411882778000803 0.9411882778000803\n","\n","\n","20 epochs\n","Predicting labels for 4,982 test sentences...\n","0.9937775993576877\n","0.9940610263028817 0.9927478603638804 0.9934040093699549 0.9937775993576877 0.9937775993576877 0.9937775993576877\n","\n","\n","**preprocessed_2_fixed**\n","\n","10epochs\n","Predicting labels for 4,982 test sentences...\n","0.735046166198314\n","0.7959920222724617 0.6613469335398029 0.7224494767706652 0.735046166198314 0.735046166198314 0.7350461661983139\n","\n","20 epochs\n","Predicting labels for 4,982 test sentences...\n","0.8898032918506624\n","0.9201835102106448 0.8570681053944263 0.8875051014573739 0.8898032918506624 0.8898032918506624 0.8898032918506624\n","\n","40 epochs\n","Predicting labels for 4,982 test sentences...\n","0.9857486953030912\n","0.9887444514901712 0.9813059505002633 0.9850111578781171 0.9857486953030912 0.9857486953030912 0.9857486953030912\n","\n","**FIRST 510 TOKENS**\n","\n","**preprocessed_2_fixed**\n","\n","Predicting labels for 4,982 test sentences...\n","0.8613006824568447\n","0.8871609770357618 0.8257470129984592 0.8553530383397178 0.8613006824568447 0.8613006824568447 0.8613006824568447"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KCOktf8SDb5L","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1674669139377,"user_tz":0,"elapsed":31,"user":{"displayName":"Thanet Markchom","userId":"00244922274630544136"}},"outputId":"b5d7a631-0813-4bd8-96be-6f783f8e8d18"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"batch_size = 6\\n\\nprediction_data = TensorDataset(train_inputs, train_masks, train_labels)\\nprediction_sampler = SequentialSampler(train_data)\\nprediction_dataloader = DataLoader(train_data, shuffle=False, sampler=None, batch_size = batch_size)\\n\\nprint('Predicting labels for {:,} test sentences...'.format(len(prediction_data)))\\nmodel.eval()\\n\\npredictions , true_labels = [], []\\nexplanations = []\\n\\nfor (step, batch) in enumerate(prediction_dataloader):\\n  batch = tuple(t.to(device) for t in batch)\\n  \\n  b_input_ids, b_input_mask, b_labels = batch\\n  #print(b_input_ids)\\n\\n  with torch.no_grad():\\n      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, output_attentions=True)\\n\\n      logits = outputs.logits\\n      attentions = outputs.attentions\\n      \\n  # Move logits and labels to CPU\\n  logits = logits.detach().cpu().numpy()\\n  label_ids = b_labels.to('cpu').numpy()\\n  attentions = attentions[0].detach().cpu().numpy()\\n  \\n  # Store predictions and true labels\\n  predictions.append(logits)\\n  true_labels.append(label_ids)\\n\\npredictions = np.concatenate(predictions, axis=0)\\ntrue_labels = np.concatenate(true_labels, axis=0)\\npred_flat = np.argmax(predictions, axis=1).flatten()\\nlabels_flat = true_labels.flatten()\\n\\nprint(flat_accuracy(predictions,true_labels))\\n\\nmacro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(pred_flat, labels_flat)\\nprint(macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1)\\n\\nprint('    DONE.')\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":35}],"source":["'''batch_size = 6\n","\n","prediction_data = TensorDataset(train_inputs, train_masks, train_labels)\n","prediction_sampler = SequentialSampler(train_data)\n","prediction_dataloader = DataLoader(train_data, shuffle=False, sampler=None, batch_size = batch_size)\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_data)))\n","model.eval()\n","\n","predictions , true_labels = [], []\n","explanations = []\n","\n","for (step, batch) in enumerate(prediction_dataloader):\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  b_input_ids, b_input_mask, b_labels = batch\n","  #print(b_input_ids)\n","\n","  with torch.no_grad():\n","      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, output_attentions=True)\n","\n","      logits = outputs.logits\n","      \n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","labels_flat = true_labels.flatten()\n","\n","print(flat_accuracy(predictions,true_labels))\n","\n","macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(pred_flat, labels_flat)\n","print(macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1)\n","\n","print('    DONE.')'''"]},{"cell_type":"markdown","metadata":{"id":"_oWVq6yHo_nC"},"source":["##evaluation on dev set\n","\n","**last 510 tokens**\n","\n","**preprocessed_2**\n","\n","10 epochs\n","Predicting labels for 994 test sentences...\n","0.5985915492957746\n","0.7012910798122065 0.5985915492957746 0.6458843353575175 0.5985915492957746 0.5985915492957746 0.5985915492957746\n","\n","20 epcohs\n","Predicting labels for 994 test sentences...\n","0.6297786720321932\n","0.6888731771984091 0.6297786720321932 0.6580017841521162 0.6297786720321932 0.6297786720321932 0.6297786720321932\n","\n","\n","**preprocessed_2_fixed**\n","\n","10 epochs\n","Predicting labels for 994 test sentences...\n","0.5382293762575453\n","0.5829905786402306 0.5382293762575453 0.5597165018957676 0.5382293762575453 0.5382293762575453 0.5382293762575453\n","\n","20 epochs\n","Predicting labels for 994 test sentences...\n","0.5553319919517102\n","0.5920909893338184 0.5553319919517102 0.5731226825438435 0.5553319919517102 0.5553319919517102 0.5553319919517102\n","\n","40 epcohs \n","Predicting labels for 994 test sentences...\n","0.545271629778672\n","0.5686426164300316 0.545271629778672 0.5567119502739849 0.545271629778672 0.545271629778672 0.545271629778672\n","\n","\n","**FIRST 510 TOKENS**\n","\n","**preprocessed_2_fixed**\n","\n","10 epochs\n","0.6287726358148893\n","0.690558351306015 0.6287726358148893 0.6582187471882317 0.6287726358148893 0.6287726358148893 0.6287726358148893\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nn0CrsHYM9fz","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1674669139378,"user_tz":0,"elapsed":29,"user":{"displayName":"Thanet Markchom","userId":"00244922274630544136"}},"outputId":"1e943066-185f-4a2f-f7a7-571c86e480b8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"batch_size = 1\\n\\n# Create the DataLoader.\\nprediction_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\\nprediction_sampler = SequentialSampler(prediction_data)\\nprediction_dataloader = DataLoader(prediction_data, shuffle=False, sampler=None, batch_size = batch_size)\\n\\nprint('Predicting labels for {:,} test sentences...'.format(len(prediction_data)))\\nmodel.eval()\\n\\npredictions , true_labels = [], []\\nexplanations = []\\n\\nfor (step, batch) in enumerate(prediction_dataloader):\\n  batch = tuple(t.to(device) for t in batch)\\n  \\n  b_input_ids, b_input_mask, b_labels = batch\\n  #print(b_input_ids)\\n\\n  with torch.no_grad():\\n      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, output_attentions=True)\\n      logits = outputs.logits\\n      attentions = outputs.attentions[0]\\n\\n  # Move logits and labels to CPU\\n  logits = logits.detach().cpu().numpy()\\n  label_ids = b_labels.to('cpu').numpy()\\n  attentions = attentions.detach().cpu().numpy()\\n  \\n  # Store predictions and true labels\\n  predictions.append(logits)\\n  true_labels.append(label_ids)\\n\\npredictions = np.concatenate(predictions, axis=0)\\ntrue_labels = np.concatenate(true_labels, axis=0)\\npred_flat = np.argmax(predictions, axis=1).flatten()\\nlabels_flat = true_labels.flatten()\\n\\nprint(flat_accuracy(predictions,true_labels))\\n\\nmacro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(pred_flat, labels_flat)\\nprint(macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1)\\n\\nprint('    DONE.')\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":36}],"source":["'''batch_size = 1\n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, shuffle=False, sampler=None, batch_size = batch_size)\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_data)))\n","model.eval()\n","\n","predictions , true_labels = [], []\n","explanations = []\n","\n","for (step, batch) in enumerate(prediction_dataloader):\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  b_input_ids, b_input_mask, b_labels = batch\n","  #print(b_input_ids)\n","\n","  with torch.no_grad():\n","      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, output_attentions=True)\n","      logits = outputs.logits\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","labels_flat = true_labels.flatten()\n","\n","print(flat_accuracy(predictions,true_labels))\n","\n","macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(pred_flat, labels_flat)\n","print(macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1)\n","\n","print('    DONE.')'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cx5vgBlrZC6l"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"nM5efG4mzU5G"},"source":["## Prediction on binary classification test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eAdWPBXvwydo","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1674669139378,"user_tz":0,"elapsed":25,"user":{"displayName":"Thanet Markchom","userId":"00244922274630544136"}},"outputId":"1f23cfa5-fa4a-4d17-a5a5-02e5f39259ad"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"batch_size = 1\\n\\n# Create the DataLoader.\\nprediction_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\\nprediction_sampler = SequentialSampler(prediction_data)\\nprediction_dataloader = DataLoader(prediction_data, shuffle=False, sampler=None, batch_size = batch_size)\\nassert len(prediction_data) == 1500\\n\\nprint('Predicting labels for {:,} test sentences...'.format(len(prediction_data)))\\nmodel.eval()\\n\\npredictions , true_labels = [], []\\nexplanations = []\\n\\nfor (step, batch) in enumerate(prediction_dataloader):\\n  batch = tuple(t.to(device) for t in batch)\\n  \\n  b_input_ids, b_input_mask, b_labels = batch\\n  #print(b_input_ids)\\n\\n  with torch.no_grad():\\n      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, output_attentions=True)\\n\\n      logits = outputs.logits\\n      attentions = outputs.attentions[0]\\n      \\n  # Move logits and labels to CPU\\n  logits = logits.detach().cpu().numpy()\\n  label_ids = b_labels.to('cpu').numpy()\\n  attentions = attentions.detach().cpu().numpy()\\n  \\n  # Store predictions and true labels\\n  predictions.append(logits)\\n  true_labels.append(label_ids)\\n\\npredictions = np.concatenate(predictions, axis=0)\\ntrue_labels = np.concatenate(true_labels, axis=0)\\npred_flat = np.argmax(predictions, axis=1).flatten()\\nlabels_flat = true_labels.flatten()\\n\\n\\nprint('    DONE.')\\n\\nvalidation_set['id2'] = labels_flat\\nvalidation_set['prediction'] = pred_flat\\nassert all(validation_set['id2'] == validation_set['id'])\\nvalidation_set.to_csv(pred_file, index=False,)\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":37}],"source":["\"\"\"batch_size = 1\n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, shuffle=False, sampler=None, batch_size = batch_size)\n","assert len(prediction_data) == 1500\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_data)))\n","model.eval()\n","\n","predictions , true_labels = [], []\n","explanations = []\n","\n","for (step, batch) in enumerate(prediction_dataloader):\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  b_input_ids, b_input_mask, b_labels = batch\n","  #print(b_input_ids)\n","\n","  with torch.no_grad():\n","      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, output_attentions=True)\n","\n","      logits = outputs.logits\n","      \n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","labels_flat = true_labels.flatten()\n","\n","\n","print('    DONE.')\n","\n","validation_set['id2'] = labels_flat\n","validation_set['prediction'] = pred_flat\n","assert all(validation_set['id2'] == validation_set['id'])\n","validation_set.to_csv(pred_file, index=False,)\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"By4GNgg1_k1Q"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"FEmiKq3fJ2Ja"},"source":["##Prediction on explanation test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQbvOT4kkPxz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674669229015,"user_tz":0,"elapsed":89658,"user":{"displayName":"Thanet Markchom","userId":"00244922274630544136"}},"outputId":"c0fb8820-f4a4-4653-8001-f2a9474bfa0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Predicting labels for 50 test sentences...\n","    DONE.\n"]}],"source":["batch_size = 1\n","window_size = 128\n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, shuffle=False, sampler=None, batch_size = batch_size)\n","assert len(prediction_data) == 50\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_data)))\n","model.eval()\n","\n","predictions , true_labels = [], []\n","explanations = []\n","\n","for (step, batch) in enumerate(prediction_dataloader):\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  b_input_ids, b_input_mask, b_labels = batch\n","  #print(b_input_ids)\n","\n","  with torch.no_grad():\n","      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, output_attentions=True)\n","\n","      logits = outputs.logits\n","      attentions = outputs.attentions[-1]\n","\n","      #max_ind = attentions[0][-1][0].detach().cpu().numpy().argmax()\n","\n","      max_inds = np.argpartition(attentions[0][-1][0].detach().cpu().numpy(), -3)[-3:]\n","     \n","      explanation_sents = []\n","      for max_ind in max_inds:\n","        selected_tokens = b_input_ids[0][max_ind-int(window_size/2):max_ind+int(window_size/2)]\n","        tokens = tokenizer.convert_ids_to_tokens(selected_tokens) \n","        #print(tokens)\n","        sent = tokenizer.convert_tokens_to_string(tokens)\n","        #print(sent)\n","        explanation_sents.append(sent)\n","\n","      explanation_sents = \" \".join(explanation_sents)\n","      explanations.append(explanation_sents) \n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  attentions = attentions.detach().cpu().numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","labels_flat = true_labels.flatten()\n","\n","print('    DONE.')\n","\n","\n","decisions = ['Accepted' if _ == 1 else 'Denied' for _ in pred_flat]\n","\n","validation_set['id2'] = labels_flat\n","validation_set['prediction'] = pred_flat\n","validation_set['decision'] = decisions\n","validation_set['explanation'] = explanations\n","assert all(validation_set['id2'] == validation_set['id'])\n","validation_set.to_csv(pred_file, index=False, columns=['uid','decision','explanation'] )"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}